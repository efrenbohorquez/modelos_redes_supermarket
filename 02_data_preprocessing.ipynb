{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos - Ventas de Supermercado\n",
    "\n",
    "Este notebook realiza el preprocesamiento de los datos de ventas de supermercado para prepararlos para el modelado predictivo. Se aplican técnicas de limpieza, transformación y codificación de variables para optimizar el rendimiento de los modelos de redes neuronales.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "1. Limpiar y transformar los datos para su uso en modelos predictivos\n",
    "2. Codificar variables categóricas\n",
    "3. Normalizar variables numéricas\n",
    "4. Crear características temporales relevantes\n",
    "5. Preparar conjuntos de datos específicos para diferentes tipos de modelos\n",
    "6. Guardar los datos procesados para su uso en notebooks posteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Agregar directorio raíz al path para importar utilidades\n",
    "sys.path.append('..')\n",
    "from utils.data_utils import load_data, save_processed_data, preprocess_data, prepare_time_series_data, create_features_for_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos originales o procesados del notebook anterior\n",
    "try:\n",
    "    # Intentar cargar datos procesados del notebook anterior\n",
    "    data_path = '../data/processed/supermarket_sales_processed.csv'\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"Datos procesados cargados correctamente.\")\n",
    "except FileNotFoundError:\n",
    "    # Si no existen, cargar datos originales\n",
    "    data_path = '../data/raw/supermarket_sales.xlsx'\n",
    "    df = load_data(data_path)\n",
    "    print(\"Datos originales cargados correctamente.\")\n",
    "\n",
    "# Mostrar primeras filas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificación y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores nulos\n",
    "print(\"Valores nulos por columna:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar duplicados\n",
    "print(f\"Número de filas duplicadas: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar tipos de datos\n",
    "print(\"Tipos de datos:\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia para no modificar el original\n",
    "data = df.copy()\n",
    "\n",
    "# Convertir fecha y hora a datetime si no lo están\n",
    "if 'DateTime' not in data.columns:\n",
    "    if 'Date' in data.columns and isinstance(data['Date'].iloc[0], str):\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "    \n",
    "    if 'Time' in data.columns:\n",
    "        data['DateTime'] = pd.to_datetime(data['Date'].astype(str) + ' ' + data['Time'])\n",
    "        \n",
    "        # Extraer componentes de fecha y hora\n",
    "        data['Day'] = data['DateTime'].dt.day\n",
    "        data['Month'] = data['DateTime'].dt.month\n",
    "        data['Year'] = data['DateTime'].dt.year\n",
    "        data['DayOfWeek'] = data['DateTime'].dt.dayofweek\n",
    "        data['DayName'] = data['DateTime'].dt.day_name()\n",
    "        data['Hour'] = data['DateTime'].dt.hour\n",
    "\n",
    "# Mostrar las columnas de fecha y hora\n",
    "date_cols = [col for col in data.columns if col in ['Date', 'Time', 'DateTime', 'Day', 'Month', 'Year', 'DayOfWeek', 'DayName', 'Hour']]\n",
    "data[date_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de Variables Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar variables categóricas\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Variables categóricas: {categorical_cols}\")\n",
    "\n",
    "# Eliminar columnas que no se usarán para el modelado\n",
    "if 'Invoice ID' in categorical_cols:\n",
    "    categorical_cols.remove('Invoice ID')\n",
    "if 'Date' in categorical_cols:\n",
    "    categorical_cols.remove('Date')\n",
    "if 'Time' in categorical_cols:\n",
    "    categorical_cols.remove('Time')\n",
    "if 'DayName' in categorical_cols:\n",
    "    categorical_cols.remove('DayName')\n",
    "\n",
    "print(f\"Variables categóricas para modelado: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear codificadores para variables categóricas\n",
    "label_encoders = {}\n",
    "data_le = data.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    data_le[col] = le.fit_transform(data_le[col])\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "    # Mostrar mapeo\n",
    "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(f\"Mapeo para {col}: {mapping}\")\n",
    "\n",
    "# Mostrar datos con codificación de etiquetas\n",
    "data_le[categorical_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear codificación one-hot\n",
    "data_onehot = data.copy()\n",
    "data_onehot = pd.get_dummies(data_onehot, columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Mostrar nuevas columnas creadas\n",
    "new_cols = [col for col in data_onehot.columns if col not in data.columns]\n",
    "print(f\"Número de nuevas columnas creadas: {len(new_cols)}\")\n",
    "print(f\"Primeras 10 nuevas columnas: {new_cols[:10]}\")\n",
    "\n",
    "# Mostrar datos con codificación one-hot\n",
    "data_onehot[new_cols[:10]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de Variables Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar variables numéricas\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Eliminar variables que no se usarán para modelado o que son objetivo\n",
    "cols_to_exclude = ['Invoice ID', 'DateTime']\n",
    "numerical_cols = [col for col in numerical_cols if col not in cols_to_exclude]\n",
    "\n",
    "print(f\"Variables numéricas para modelado: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear normalizadores\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Aplicar StandardScaler\n",
    "data_std = data.copy()\n",
    "data_std[numerical_cols] = standard_scaler.fit_transform(data_std[numerical_cols])\n",
    "\n",
    "# Aplicar MinMaxScaler\n",
    "data_minmax = data.copy()\n",
    "data_minmax[numerical_cols] = minmax_scaler.fit_transform(data_minmax[numerical_cols])\n",
    "\n",
    "# Mostrar datos normalizados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Datos con StandardScaler\n",
    "axes[0].set_title('Datos con StandardScaler')\n",
    "sns.heatmap(data_std[numerical_cols].head(10), cmap='viridis', ax=axes[0])\n",
    "\n",
    "# Datos con MinMaxScaler\n",
    "axes[1].set_title('Datos con MinMaxScaler')\n",
    "sns.heatmap(data_minmax[numerical_cols].head(10), cmap='viridis', ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de Datos para Diferentes Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Datos para Modelos MLP y Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variables objetivo para diferentes tareas\n",
    "target_variables = {\n",
    "    'ventas_totales': 'Total',\n",
    "    'calificacion_cliente': 'Rating',\n",
    "    'ingresos_brutos': 'gross income'\n",
    "}\n",
    "\n",
    "# Preparar datos para modelos MLP y baseline (regresión, árboles)\n",
    "mlp_datasets = {}\n",
    "\n",
    "for target_name, target_col in target_variables.items():\n",
    "    print(f\"\\nPreparando datos para predecir {target_name} ({target_col})...\")\n",
    "    \n",
    "    # Usar función de utilidad para preprocesar datos\n",
    "    X_train, X_test, y_train, y_test, preprocessor = preprocess_data(\n",
    "        data, \n",
    "        target_variable=target_col, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Guardar datasets\n",
    "    mlp_datasets[target_name] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'preprocessor': preprocessor\n",
    "    }\n",
    "    \n",
    "    print(f\"Forma de X_train: {X_train.shape}\")\n",
    "    print(f\"Forma de X_test: {X_test.shape}\")\n",
    "    print(f\"Número de características: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Datos para Modelos LSTM (Series Temporales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para series temporales (LSTM)\n",
    "lstm_datasets = {}\n",
    "\n",
    "# Ordenar datos por fecha\n",
    "data_sorted = data.sort_values('DateTime')\n",
    "\n",
    "# Preparar datos para diferentes objetivos\n",
    "for target_name, target_col in target_variables.items():\n",
    "    print(f\"\\nPreparando datos de series temporales para predecir {target_name} ({target_col})...\")\n",
    "    \n",
    "    # Crear datos diarios agregados\n",
    "    daily_data = data_sorted.groupby('Date')[target_col].sum().reset_index()\n",
    "    daily_values = daily_data[target_col].values\n",
    "    \n",
    "    # Usar diferentes longitudes de secuencia\n",
    "    for seq_length in [7, 14]:\n",
    "        # Usar función de utilidad para preparar datos de series temporales\n",
    "        X_train, X_test, y_train, y_test = prepare_time_series_data(\n",
    "            daily_data, \n",
    "            target_col=target_col, \n",
    "            sequence_length=seq_length\n",
    "        )\n",
    "        \n",
    "        # Guardar datasets\n",
    "        lstm_datasets[f\"{target_name}_seq{seq_length}\"] = {\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'sequence_length': seq_length\n",
    "        }\n",
    "        \n",
    "        print(f\"Secuencia de {seq_length} días:\")\n",
    "        print(f\"Forma de X_train: {X_train.shape}\")\n",
    "        print(f\"Forma de X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Datos para Modelos CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para modelos CNN\n",
    "cnn_datasets = {}\n",
    "\n",
    "for target_name, target_col in target_variables.items():\n",
    "    print(f\"\\nPreparando datos para CNN para predecir {target_name} ({target_col})...\")\n",
    "    \n",
    "    # Usar función de utilidad para preparar datos para CNN\n",
    "    X_train, X_test, y_train, y_test, preprocessor = create_features_for_cnn(\n",
    "        data, \n",
    "        target_col=target_col\n",
    "    )\n",
    "    \n",
    "    # Guardar datasets\n",
    "    cnn_datasets[target_name] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'preprocessor': preprocessor\n",
    "    }\n",
    "    \n",
    "    print(f\"Forma de X_train: {X_train.shape}\")\n",
    "    print(f\"Forma de X_test: {X_test.shape}\")\n",
    "    print(f\"Dimensiones de la matriz: {preprocessor['height']}x{preprocessor['width']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio para datos procesados si no existe\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Guardar datos procesados\n",
    "processed_data_path = '../data/processed/supermarket_sales_processed.csv'\n",
    "save_processed_data(data, processed_data_path)\n",
    "\n",
    "# Guardar datasets para diferentes modelos\n",
    "joblib.dump(mlp_datasets, '../data/processed/mlp_datasets.joblib')\n",
    "joblib.dump(lstm_datasets, '../data/processed/lstm_datasets.joblib')\n",
    "joblib.dump(cnn_datasets, '../data/processed/cnn_datasets.joblib')\n",
    "\n",
    "print(\"Todos los datasets han sido guardados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del Preprocesamiento\n",
    "\n",
    "En este notebook, hemos realizado las siguientes tareas de preprocesamiento:\n",
    "\n",
    "1. **Limpieza de Datos**:\n",
    "   - Verificación y manejo de valores nulos\n",
    "   - Verificación y manejo de duplicados\n",
    "   - Conversión de tipos de datos\n",
    "\n",
    "2. **Transformación de Variables**:\n",
    "   - Creación de características temporales (día, mes, año, día de la semana, hora)\n",
    "   - Codificación de variables categóricas (Label Encoding y One-Hot Encoding)\n",
    "   - Normalización de variables numéricas (StandardScaler y MinMaxScaler)\n",
    "\n",
    "3. **Preparación de Datos para Diferentes Modelos**:\n",
    "   - Datos para modelos MLP y baseline (regresión, árboles)\n",
    "   - Datos para modelos LSTM (series temporales)\n",
    "   - Datos para modelos CNN\n",
    "\n",
    "4. **Almacenamiento de Datos Procesados**:\n",
    "   - Guardado de datos procesados en formato CSV\n",
    "   - Guardado de datasets específicos para cada tipo de modelo\n",
    "\n",
    "Estos datos procesados serán utilizados en los siguientes notebooks para entrenar y evaluar diferentes modelos de redes neuronales y técnicas de
(Content truncated due to size limit. Use line ranges to read in chunks)
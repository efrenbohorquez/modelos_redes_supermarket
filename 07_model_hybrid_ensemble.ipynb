{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Híbridos y Ensemble - Ventas de Supermercado\n",
    "\n",
    "Este notebook implementa modelos híbridos y ensemble para la predicción de ventas y otros indicadores clave en datos de supermercado. Los modelos híbridos combinan diferentes arquitecturas de redes neuronales, mientras que los modelos ensemble integran múltiples modelos para obtener predicciones más robustas y precisas.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "1. Cargar y preparar los datos preprocesados\n",
    "2. Implementar un modelo híbrido que combine características de MLP, LSTM y CNN\n",
    "3. Implementar modelos ensemble que integren múltiples modelos\n",
    "4. Evaluar el rendimiento de los modelos\n",
    "5. Comparar con modelos individuales previamente entrenados\n",
    "6. Guardar los modelos entrenados para su uso posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Configuración de TensorFlow\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Agregar directorio raíz al path para importar utilidades\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Datos Preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datasets preprocesados\n",
    "try:\n",
    "    mlp_datasets = joblib.load('../data/processed/mlp_datasets.joblib')\n",
    "    lstm_datasets = joblib.load('../data/processed/lstm_datasets.joblib')\n",
    "    cnn_datasets = joblib.load('../data/processed/cnn_datasets.joblib')\n",
    "    print(\"Datasets preprocesados cargados correctamente.\")\n",
    "    \n",
    "    # Mostrar información de los datasets disponibles\n",
    "    print(f\"\\nDatasets MLP disponibles: {list(mlp_datasets.keys())}\")\n",
    "    print(f\"Datasets LSTM disponibles: {list(lstm_datasets.keys())}\")\n",
    "    print(f\"Datasets CNN disponibles: {list(cnn_datasets.keys())}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No se encontraron los datasets preprocesados. Ejecute primero el notebook de preprocesamiento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Modelos Previamente Entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar resultados de modelos previamente entrenados\n",
    "try:\n",
    "    mlp_results = joblib.load('../models/mlp/results/mlp_results.joblib')\n",
    "    lstm_results = joblib.load('../models/lstm/results/lstm_results.joblib')\n",
    "    cnn_results = joblib.load('../models/cnn/results/cnn_results.joblib')\n",
    "    baseline_results = joblib.load('../models/baseline/results/baseline_results.joblib')\n",
    "    print(\"Resultados de modelos previamente entrenados cargados correctamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Advertencia: No se encontraron resultados de algunos modelos previamente entrenados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelos previamente entrenados\n",
    "models_loaded = {}\n",
    "\n",
    "try:\n",
    "    # Cargar modelo MLP\n",
    "    if 'ventas_totales' in mlp_results:\n",
    "        mlp_model_path = mlp_results['ventas_totales']['model_path']\n",
    "        models_loaded['mlp'] = load_model(mlp_model_path)\n",
    "        print(f\"Modelo MLP cargado desde {mlp_model_path}\")\n",
    "    \n",
    "    # Cargar modelo LSTM\n",
    "    if 'ventas_totales_seq7' in lstm_results:\n",
    "        lstm_model_path = lstm_results['ventas_totales_seq7']['model_path']\n",
    "        models_loaded['lstm'] = load_model(lstm_model_path)\n",
    "        print(f\"Modelo LSTM cargado desde {lstm_model_path}\")\n",
    "    \n",
    "    # Cargar modelo CNN\n",
    "    if 'ventas_totales' in cnn_results:\n",
    "        cnn_model_path = cnn_results['ventas_totales']['model_path']\n",
    "        models_loaded['cnn'] = load_model(cnn_model_path)\n",
    "        print(f\"Modelo CNN cargado desde {cnn_model_path}\")\n",
    "    \n",
    "    # Cargar modelos baseline\n",
    "    if os.path.exists('../models/baseline/models/best_tree_model.joblib'):\n",
    "        models_loaded['tree'] = joblib.load('../models/baseline/models/best_tree_model.joblib')\n",
    "        print(\"Modelo de árbol cargado correctamente\")\n",
    "    \n",
    "    if os.path.exists('../models/baseline/models/rf_optimized_model.joblib'):\n",
    "        models_loaded['rf'] = joblib.load('../models/baseline/models/rf_optimized_model.joblib')\n",
    "        print(\"Modelo Random Forest optimizado cargado correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar modelos: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Modelo Híbrido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_model(mlp_input_shape, lstm_input_shape, cnn_input_shape, \n",
    "                        mlp_units=[64, 32], lstm_units=[50], cnn_filters=[32, 64],\n",
    "                        dropout_rate=0.3, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Crea un modelo híbrido que combina MLP, LSTM y CNN.\n",
    "    \n",
    "    Args:\n",
    "        mlp_input_shape: Forma de entrada para la parte MLP.\n",
    "        lstm_input_shape: Forma de entrada para la parte LSTM.\n",
    "        cnn_input_shape: Forma de entrada para la parte CNN.\n",
    "        mlp_units, lstm_units, cnn_filters: Parámetros de arquitectura.\n",
    "        dropout_rate: Tasa de dropout para regularización.\n",
    "        learning_rate: Tasa de aprendizaje para el optimizador.\n",
    "        \n",
    "    Returns:\n",
    "        model: Modelo híbrido compilado.\n",
    "    \"\"\"\n",
    "    # Rama MLP\n",
    "    mlp_input = Input(shape=mlp_input_shape, name='mlp_input')\n",
    "    mlp = Dense(mlp_units[0], activation='relu')(mlp_input)\n",
    "    mlp = BatchNormalization()(mlp)\n",
    "    mlp = Dropout(dropout_rate)(mlp)\n",
    "    \n",
    "    for units in mlp_units[1:]:\n",
    "        mlp = Dense(units, activation='relu')(mlp)\n",
    "        mlp = BatchNormalization()(mlp)\n",
    "        mlp = Dropout(dropout_rate)(mlp)\n",
    "    \n",
    "    # Rama LSTM\n",
    "    lstm_input = Input(shape=lstm_input_shape, name='lstm_input')\n",
    "    lstm = LSTM(lstm_units[0])(lstm_input)\n",
    "    lstm = BatchNormalization()(lstm)\n",
    "    lstm = Dropout(dropout_rate)(lstm)\n",
    "    \n",
    "    # Rama CNN\n",
    "    cnn_input = Input(shape=cnn_input_shape, name='cnn_input')\n",
    "    cnn = Conv2D(cnn_filters[0], kernel_size=3, activation='relu', padding='same')(cnn_input)\n",
    "    cnn = BatchNormalization()(cnn)\n",
    "    cnn = MaxPooling2D(pool_size=(2, 2))(cnn)\n",
    "    cnn = Dropout(dropout_rate/2)(cnn)\n",
    "    \n",
    "    for filter_size in cnn_filters[1:]:\n",
    "        cnn = Conv2D(filter_size, kernel_size=3, activation='relu', padding='same')(cnn)\n",
    "        cnn = BatchNormalization()(cnn)\n",
    "        cnn = MaxPooling2D(pool_size=(2, 2))(cnn)\n",
    "        cnn = Dropout(dropout_rate/2)(cnn)\n",
    "    \n",
    "    cnn = Flatten()(cnn)\n",
    "    \n",
    "    # Concatenar las salidas de las tres ramas\n",
    "    combined = Concatenate()([mlp, lstm, cnn])\n",
    "    \n",
    "    # Capas densas finales\n",
    "    x = Dense(64, activation='relu')(combined)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Capa de salida\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Crear modelo\n",
    "    model = Model(inputs=[mlp_input, lstm_input, cnn_input], outputs=output)\n",
    "    \n",
    "    # Compilar modelo\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para el modelo híbrido\n",
    "dataset_name = 'ventas_totales'\n",
    "if dataset_name in mlp_datasets and 'ventas_totales_seq7' in lstm_datasets and dataset_name in cnn_datasets:\n",
    "    # Datos MLP\n",
    "    X_train_mlp = mlp_datasets[dataset_name]['X_train']\n",
    "    X_test_mlp = mlp_datasets[dataset_name]['X_test']\n",
    "    y_train = mlp_datasets[dataset_name]['y_train']\n",
    "    y_test = mlp_datasets[dataset_name]['y_test']\n",
    "    \n",
    "    # Datos LSTM\n",
    "    X_train_lstm = lstm_datasets['ventas_totales_seq7']['X_train']\n",
    "    X_test_lstm = lstm_datasets['ventas_totales_seq7']['X_test']\n",
    "    \n",
    "    # Datos CNN\n",
    "    X_train_cnn = cnn_datasets[dataset_name]['X_train']\n",
    "    X_test_cnn = cnn_datasets[dataset_name]['X_test']\n",
    "    \n",
    "    # Verificar que las longitudes coincidan\n",
    "    min_train_samples = min(len(X_train_mlp), len(X_train_lstm), len(X_train_cnn))\n",
    "    min_test_samples = min(len(X_test_mlp), len(X_test_lstm), len(X_test_cnn))\n",
    "    \n",
    "    print(f\"Muestras de entrenamiento disponibles: MLP={len(X_train_mlp)}, LSTM={len(X_train_lstm)}, CNN={len(X_train_cnn)}\")\n",
    "    print(f\"Muestras de prueba disponibles: MLP={len(X_test_mlp)}, LSTM={len(X_test_lstm)}, CNN={len(X_test_cnn)}\")\n",
    "    print(f\"Usando {min_train_samples} muestras para entrenamiento y {min_test_samples} para prueba\")\n",
    "    \n",
    "    # Ajustar tamaños\n",
    "    X_train_mlp = X_train_mlp[:min_train_samples]\n",
    "    X_train_lstm = X_train_lstm[:min_train_samples]\n",
    "    X_train_cnn = X_train_cnn[:min_train_samples]\n",
    "    y_train = y_train[:min_train_samples]\n",
    "    \n",
    "    X_test_mlp = X_test_mlp[:min_test_samples]\n",
    "    X_test_lstm = X_test_lstm[:min_test_samples]\n",
    "    X_test_cnn = X_test_cnn[:min_test_samples]\n",
    "    y_test = y_test[:min_test_samples]\n",
    "    \n",
    "    # Crear y entrenar modelo híbrido\n",
    "    print(\"\\nCreando modelo híbrido...\")\n",
    "    \n",
    "    # Obtener formas de entrada\n",
    "    mlp_input_shape = X_train_mlp.shape[1:]\n",
    "    lstm_input_shape = X_train_lstm.shape[1:]\n",
    "    cnn_input_shape = X_train_cnn.shape[1:]\n",
    "    \n",
    "    print(f\"Forma de entrada MLP: {mlp_input_shape}\")\n",
    "    print(f\"Forma de entrada LSTM: {lstm_input_shape}\")\n",
    "    print(f\"Forma de entrada CNN: {cnn_input_shape}\")\n",
    "    \n",
    "    # Crear modelo híbrido\n",
    "    hybrid_model = create_hybrid_model(\n",
    "        mlp_input_shape=mlp_input_shape,\n",
    "        lstm_input_shape=lstm_input_shape,\n",
    "        cnn_input_shape=cnn_input_shape,\n",
    "        mlp_units=[128, 64],\n",
    "        lstm_units=[64],\n",
    "        cnn_filters=[32, 64],\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    # Mostrar resumen del modelo\n",
    "    hybrid_model.summary()\n",
    "    \n",
    "    # Crear directorio para modelos si no existe\n",
    "    models_dir = '../models/hybrid'\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Callbacks para entrenamiento\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001),\n",
    "        ModelCheckpoint(f\"{models_dir}/hybrid_model.h5\", save_best_only=True, monitor='val_loss')\n",
    "    ]\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"\\nEntrenando modelo híbrido...\")\n",
    "    start_time = time.time()\n",
    "    history = hybrid_model.fit(\n",
    "        [X_train_mlp, X_train_lstm, X_train_cnn], y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    y_pred = hybrid_model.predict([X_test_mlp, X_test_lstm, X_test_cnn]).flatten()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Mostrar métricas\n",
    "    print(f\"\\nResultados del modelo híbrido:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"Tiempo de entrenamiento: {training_time:.2f} segundos\")\n",
    "    \n",
    "    # Visualizar historial de entrenamiento\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Validación')\n",
    "    plt.title('Pérdida durante el Entrenamiento')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Entrenamiento')\n",
    "    plt.plot(history.history['val_mae'], label='Validación')\n",
    "    plt.title('Error Absoluto Medio durante el Entrenamiento')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.suptitle('Historial de Entrenamiento - Modelo Híbrido')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualizar predicciones\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    min_val = min(np.min(y_test), np.min(y_pred))\n",
    "    max_val = max(np.max(y_test), np.max(y_pred))\n",
    "  
(Content truncated due to size limit. Use line ranges to read in chunks)
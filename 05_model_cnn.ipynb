{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Red Neuronal Convolucional (CNN) - Ventas de Supermercado\n",
    "\n",
    "Este notebook implementa un modelo de Red Neuronal Convolucional (CNN) para la predicción de ventas y otros indicadores clave en datos de supermercado. Aunque las CNN son tradicionalmente utilizadas para datos de imagen, han demostrado ser efectivas para extraer patrones complejos en datos tabulares cuando se estructuran adecuadamente.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "1. Cargar y preparar los datos preprocesados para CNN\n",
    "2. Diseñar arquitecturas de redes neuronales convolucionales\n",
    "3. Entrenar modelos para diferentes variables objetivo\n",
    "4. Evaluar el rendimiento de los modelos\n",
    "5. Visualizar resultados y realizar predicciones\n",
    "6. Guardar los modelos entrenados para su uso posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Configuración de TensorFlow\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Agregar directorio raíz al path para importar utilidades\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Datos Preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datasets preprocesados para CNN\n",
    "try:\n",
    "    cnn_datasets = joblib.load('../data/processed/cnn_datasets.joblib')\n",
    "    print(\"Datasets para CNN cargados correctamente.\")\n",
    "    \n",
    "    # Mostrar información de los datasets disponibles\n",
    "    print(f\"\\nDatasets disponibles: {list(cnn_datasets.keys())}\")\n",
    "    \n",
    "    for name, dataset in cnn_datasets.items():\n",
    "        print(f\"\\nDataset: {name}\")\n",
    "        print(f\"X_train shape: {dataset['X_train'].shape}\")\n",
    "        print(f\"X_test shape: {dataset['X_test'].shape}\")\n",
    "        print(f\"y_train shape: {dataset['y_train'].shape}\")\n",
    "        print(f\"y_test shape: {dataset['y_test'].shape}\")\n",
    "        print(f\"Dimensiones de la matriz: {dataset['preprocessor']['height']}x{dataset['preprocessor']['width']}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No se encontraron los datasets preprocesados. Ejecute primero el notebook de preprocesamiento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Utilidad para Modelos CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, filters=[32, 64], kernel_size=3, \n",
    "                     dense_units=[128, 64], dropout_rate=0.3, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Crea un modelo de Red Neuronal Convolucional (CNN).\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Forma de entrada (height, width, channels).\n",
    "        filters (list): Lista con el número de filtros en cada capa convolucional.\n",
    "        kernel_size (int): Tamaño del kernel para las capas convolucionales.\n",
    "        dense_units (list): Lista con el número de unidades en cada capa densa.\n",
    "        dropout_rate (float): Tasa de dropout para regularización.\n",
    "        learning_rate (float): Tasa de aprendizaje para el optimizador.\n",
    "        \n",
    "    Returns:\n",
    "        model: Modelo de Keras compilado.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Primera capa convolucional\n",
    "    model.add(Conv2D(filters[0], kernel_size=kernel_size, activation='relu', \n",
    "                     padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    \n",
    "    # Capas convolucionales adicionales\n",
    "    for filter_size in filters[1:]:\n",
    "        model.add(Conv2D(filter_size, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(dropout_rate/2))\n",
    "    \n",
    "    # Aplanar para capas densas\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Capas densas\n",
    "    for units in dense_units:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Capa de salida (regresión)\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # Compilar modelo\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(X_train, y_train, X_test, y_test, model_name, \n",
    "                          filters=[32, 64], kernel_size=3, dense_units=[128, 64], \n",
    "                          dropout_rate=0.3, learning_rate=0.001, batch_size=32, epochs=100):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa un modelo CNN.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train, X_test, y_test: Datos de entrenamiento y prueba.\n",
    "        model_name (str): Nombre para guardar el modelo.\n",
    "        filters, kernel_size, dense_units, dropout_rate, learning_rate: Parámetros del modelo.\n",
    "        batch_size (int): Tamaño del lote para entrenamiento.\n",
    "        epochs (int): Número máximo de épocas para entrenamiento.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Resultados del entrenamiento y evaluación.\n",
    "    \"\"\"\n",
    "    # Crear directorio para modelos si no existe\n",
    "    models_dir = '../models/cnn'\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Obtener forma de entrada\n",
    "    input_shape = X_train.shape[1:]\n",
    "    \n",
    "    # Crear modelo\n",
    "    model = create_cnn_model(\n",
    "        input_shape=input_shape,\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dense_units=dense_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Callbacks para entrenamiento\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001),\n",
    "        ModelCheckpoint(f\"{models_dir}/{model_name}.h5\", save_best_only=True, monitor='val_loss')\n",
    "    ]\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'history': history.history,\n",
    "        'y_pred': y_pred,\n",
    "        'metrics': {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        },\n",
    "        'training_time': training_time,\n",
    "        'model_path': f\"{models_dir}/{model_name}.h5\"\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_training_history(history, title=\"Historial de Entrenamiento\"):\n",
    "    \"\"\"\n",
    "    Visualiza el historial de entrenamiento del modelo.\n",
    "    \n",
    "    Args:\n",
    "        history (dict): Historial de entrenamiento del modelo.\n",
    "        title (str): Título para el gráfico.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Gráfico de pérdida\n",
    "    ax1.plot(history['loss'], label='Entrenamiento')\n",
    "    ax1.plot(history['val_loss'], label='Validación')\n",
    "    ax1.set_title('Pérdida durante el Entrenamiento')\n",
    "    ax1.set_xlabel('Época')\n",
    "    ax1.set_ylabel('Pérdida (MSE)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Gráfico de MAE\n",
    "    ax2.plot(history['mae'], label='Entrenamiento')\n",
    "    ax2.plot(history['val_mae'], label='Validación')\n",
    "    ax2.set_title('Error Absoluto Medio durante el Entrenamiento')\n",
    "    ax2.set_xlabel('Época')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(y_true, y_pred, title=\"Predicciones vs Valores Reales\"):\n",
    "    \"\"\"\n",
    "    Visualiza las predicciones vs valores reales.\n",
    "    \n",
    "    Args:\n",
    "        y_true (array): Valores reales.\n",
    "        y_pred (array): Valores predichos.\n",
    "        title (str): Título para el gráfico.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Gráfico de dispersión\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    \n",
    "    # Línea de referencia (predicción perfecta)\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Valores Reales')\n",
    "    plt.ylabel('Predicciones')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de Datos de Entrada para CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar cómo se ven los datos de entrada para CNN\n",
    "dataset_name = 'ventas_totales'\n",
    "if dataset_name in cnn_datasets:\n",
    "    # Obtener algunos ejemplos\n",
    "    X_sample = cnn_datasets[dataset_name]['X_train'][:5]\n",
    "    \n",
    "    # Visualizar cada ejemplo\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(X_sample[i, :, :, 0], cmap='viridis')\n",
    "        ax.set_title(f'Ejemplo {i+1}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.colorbar(im, ax=axes, orientation='horizontal', pad=0.1)\n",
    "    plt.suptitle('Visualización de Datos de Entrada para CNN')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Error: No se encontró el dataset {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y Evaluación de Modelos CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modelo CNN para Predicción de Ventas Totales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener datos para predicción de ventas totales\n",
    "dataset_name = 'ventas_totales'\n",
    "if dataset_name in cnn_datasets:\n",
    "    X_train = cnn_datasets[dataset_name]['X_train']\n",
    "    X_test = cnn_datasets[dataset_name]['X_test']\n",
    "    y_train = cnn_datasets[dataset_name]['y_train']\n",
    "    y_test = cnn_datasets[dataset_name]['y_test']\n",
    "    \n",
    "    print(f\"Entrenando modelo CNN para {dataset_name}...\")\n",
    "    \n",
    "    # Definir arquitectura del modelo\n",
    "    filters = [32, 64, 128]\n",
    "    kernel_size = 3\n",
    "    dense_units = [256, 128, 64]\n",
    "    dropout_rate = 0.3\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 32\n",
    "    epochs = 100\n",
    "    \n",
    "    # Entrenar y evaluar modelo\n",
    "    results_ventas = train_and_evaluate_cnn(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        model_name=f\"cnn_{dataset_name}\",\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dense_units=dense_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    # Mostrar métricas\n",
    "    print(f\"\\nResultados para {dataset_name}:\")\n",
    "    print(f\"MSE: {results_ventas['metrics']['mse']:.4f}\")\n",
    "    print(f\"RMSE: {results_ventas['metrics']['rmse']:.4f}\")\n",
    "    print(f\"MAE: {results_ventas['metrics']['mae']:.4f}\")\n",
    "    print(f\"R²: {results_ventas['metrics']['r2']:.4f}\")\n",
    "    print(f\"Tiempo de entrenamiento: {results_ventas['training_time']:.2f} segundos\")\n",
    "    \n",
    "    # Visualizar historial de entrenamiento\n",
    "    plot_training_history(results_ventas['history'], \n",
    "                         title=f\"Historial de Entrenamiento - CNN para {dataset_name}\")\n",
    "    \n",
    "    # Visualizar predicciones\n",
    "    plot_predictions(y_test, results_ventas['y_pred'], \n",
    "                    title=f\"Predicciones vs Valores Reales - {dataset_name}\")\n",
    "else:\n",
    "    print(f\"Error: No se encontró el dataset {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modelo CNN para Predicción de Calificación del Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener datos para predicción de calificación del cliente\n",
    "dataset_name = 'calificacion_cliente'\n",
    "if dataset_name in cnn_datasets:\n",
    "    X_train = cnn_datasets[dataset_name]['X_train']\n",
    "    X_test = cnn_datasets[dataset_name]['X_test']\n",
    "    y_train = cnn_datasets[dataset_name]['y_train']\n",
    "    y_test = cnn_datasets[dataset_name]['y_test']\n",
    "    \n",
    "    print(f\"Entrenando modelo CNN para {dataset_name}...\")\n",
    "    \n",
    "    # Defini
(Content truncated due to size limit. Use line ranges to read in chunks)